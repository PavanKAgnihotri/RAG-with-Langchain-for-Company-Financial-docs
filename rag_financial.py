# -*- coding: utf-8 -*-
"""RAG-financial.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kx45p5cPbd3ueTmtIQUFJn2LnTnQ6aZy
"""

!pip install -U langchain-community
!pip install pymupdf
!pip install langchain langchain-huggingface pymupdf faiss-cpu
!pip install langchain faiss-cpu sentence-transformers pymupdf

from langchain.document_loaders import PyMuPDFLoader
from pathlib import Path
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from transformers import pipeline
from langchain.llms import HuggingFacePipeline

def load_pdfs_from_directory(directory):
    all_docs = []
    pdf_files = list(Path(directory).glob("*.pdf"))
    for path in pdf_files:
        loader = PyMuPDFLoader(str(path))
        docs = loader.load()
        all_docs.extend(docs)
    return all_docs

def split_documents(documents):
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    return splitter.split_documents(documents)

# Step 3: Create FAISS vector store
def create_faiss_index(docs):
    embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-small-en-v1.5")
    vectordb = FAISS.from_documents(docs, embeddings)
    return vectordb

# Step 4: Run a retrieval query
def query_index(index, query, top_k=5):
    results = index.similarity_search(query, k=top_k)

    return results

if __name__ == "__main__":
    docs = load_pdfs_from_directory("sample_data/financial-docs")
    print(f"âœ… Loaded {len(docs)} documents.")

    split_docs = split_documents(docs)
    print(f"ðŸ§© Split into {len(split_docs)} chunks.")

    vectordb = create_faiss_index(split_docs)
    print("ðŸ“¦ FAISS vector store created.")

    vectordb.save_local("faiss_index")

from langchain.prompts import PromptTemplate

def create_prompt(context, question):
    prompt_template = PromptTemplate(
        input_variables=["context", "question"],
        template="""You are a helpful financial assistant.

    Answer the question using the following context from company financial reports:

    {context}

    Question: {question}
    Answer:"""
    )
    return prompt_template.format(context=context, question=question)

def format_docs_as_context(retrieved_docs):
    """Format retrieved documents into a readable context string."""
    context_parts = []

    for i, doc in enumerate(retrieved_docs):
        # `doc.page_content` is standard in LangChain docs
        context_parts.append(f"Document {i+1}:\n{doc.page_content.strip()}")

    return "\n\n".join(context_parts)

from langchain.chains import LLMChain
from langchain_core.prompts import PromptTemplate # Import PromptTemplate
from transformers import pipeline # Import pipeline

def chatbot_financial_assistant():
    print("Chatbot: Hello! How can I help you today?\nEnter â€˜quitâ€™ or â€˜exitâ€™ to end the session")
    index_directory = "faiss_index"

    # Create the same embeddings model used to create the index
    embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-small-en-v1.5")

    # Load the FAISS index
    loaded_vectordb = FAISS.load_local(index_directory, embeddings, allow_dangerous_deserialization=True)

    print(f"âœ… FAISS index loaded successfully from '{index_directory}'.")


    while True:
      query = input("You: ")
      if query.lower() in ["exit", "quit"]:
          print("Chatbot: Goodbye!")
          break

      retrieved_docs = query_index(loaded_vectordb, query)
      context = format_docs_as_context(retrieved_docs)

      # Create the HuggingFacePipeline with model_kwargs for temperature (removed max_new_tokens)
      pipe = pipeline(
          "text2text-generation",
          model="google/flan-t5-base",
          model_kwargs={"temperature": 0.2} # max_new_tokens removed from here
      )
      llm = HuggingFacePipeline(pipeline=pipe)

      # Create the PromptTemplate object inside the loop using the template string
      prompt_template = PromptTemplate(
          input_variables=["context", "question"],
          template="""You are a helpful financial assistant.

    Answer the question using the following context from company financial reports:

    {context}

    Question: {question}
    Answer:"""
      )


      # Create LLMChain with the PromptTemplate object
      rag_chain = LLMChain(
          llm=llm,
          prompt=prompt_template # Pass the PromptTemplate object
      )

      # Run the chain with context and query
      response = rag_chain.run({
          "context": context,
          "question": query
      })

      print("Chatbot:", response)

chatbot_financial_assistant()

##NOT USED

import os
from langchain.llms import HuggingFaceHub

# Replace "YOUR_HUGGINGFACE_API_TOKEN" with your actual token
os.environ["HUGGINGFACEHUB_API_TOKEN"] = ""

llm = HuggingFaceHub(
    repo_id="google/flan-t5-base",
    task="text2text-generation",
    model_kwargs={"temperature": 0.2, "max_new_tokens": 512}
)

#NOT USED but works

from langchain.chains import LLMChain
def chatbot_financial_assistant_old():
    print("Chatbot: Hello! How can I help you today?\nEnter â€˜quitâ€™ or â€˜exitâ€™ to end the session")
    index_directory = "faiss_index"

    # Create the same embeddings model used to create the index
    embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-small-en-v1.5")

    # Load the FAISS index
    loaded_vectordb = FAISS.load_local(index_directory, embeddings, allow_dangerous_deserialization=True)

    print(f"âœ… FAISS index loaded successfully from '{index_directory}'.")

    while True:
      query = input("You: ")
      if query.lower() in ["exit", "quit"]:
          print("Chatbot: Goodbye!")
          break

      retrieved_docs = query_index(loaded_vectordb, query)
      context = format_docs_as_context(retrieved_docs)
      prompt = create_prompt(context, query)
      pipe = pipeline(
          "text2text-generation",
          model="google/flan-t5-base",
          temperature=0.2,# or flan-t5-small for faster CPU inference
          max_new_tokens=512
      )
      llm = HuggingFacePipeline(pipeline=pipe)

      response = llm(prompt)
      print("Chatbot:", response)

chatbot_financial_assistant_old()

